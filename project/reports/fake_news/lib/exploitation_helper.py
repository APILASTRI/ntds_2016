import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.contrib import learn


def preprocessing(dataframe):
    mask_true = np.array(dataframe.rating == 'mostly true')
    mask_false = np.array(dataframe.rating == 'mostly false')

    # Extract text and labels
    x_text = dataframe.message.fillna('')
    y = dataframe.rating

    # Keep only two labels
    x_true = x_text[mask_true]
    x_false = x_text[mask_false]
    y_true = y[mask_true]
    y_false = y[mask_false]
    y_true[:] = 0
    y_false[:] = 1


    # Reassemble the filtered data
    x_text = pd.concat([x_true, x_false])
    y_target = pd.concat([y_true, y_false])

    # Shuffle (fixed seed)
    x_text = x_text.sample(frac=1, random_state=0)
    y_target = y_target.sample(frac=1, random_state=0)

    # Pad the messages such that they all have the same length
    max_document_length = x_text.map(lambda s: s.split(" ")).map(len).max()
    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)

    x = np.array(list(vocab_processor.fit_transform(x_text)))
    y_array = y_target.as_matrix()
    y = np.zeros((len(y_array), 2), dtype=int)
    for i, a in enumerate(y_array):
        if a == 1:
            y[i, :] = [0, 1]
        else:
            y[i, :] = [1, 0]

    return x, y, vocab_processor


def compute_confusion_metrics(predicted, actual):
    # Count true positives, true negatives, false positives and false negatives.
    tp = tf.count_nonzero(predicted * actual)
    tn = tf.count_nonzero((predicted - 1) * (actual - 1))
    fp = tf.count_nonzero(predicted * (actual - 1))
    fn = tf.count_nonzero((predicted - 1) * actual)

    # Calculate accuracy, precision, recall and F1 score.
    # accuracy = (tp + tn) / (tp + fp + fn + tn)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    fmeasure = (2 * precision * recall) / (precision + recall)

    return precision, recall, fmeasure

def batch_iter(data, batch_size, num_epochs, shuffle=True):
    """
    Generates a batch iterator for a dataset.
    """
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1
    for epoch in range(num_epochs):
        # Shuffle the data at each epoch
        if shuffle:
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_index:end_index]
