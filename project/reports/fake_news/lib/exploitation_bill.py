import numpy as np

from sklearn.model_selection import KFold
from sklearn import naive_bayes
from sklearn import neighbors
from sklearn import metrics


def knn_classification(X, y, n_neighbors=1, cv_num_split=5):
    """
    KNN classifier with embedded cross-validation

    Arguments:
    ----------
    X : array-like
        Data matrix (shape N x P, where N is the number of data points and P is
        the number of features)
    y : array-like
        Target vector (shape N)
    n_neighbors : int
        Number of neighbors tu use for majority vote in the KNN classifier
    cv_num_split : int
        Number of splits for cross validation

    Returns:
    --------
    clf : sklearn.neighbors.KNeighborsClassifier
        Trained classifier
    confmat : array-like
        Confusiom matrix averaged over cross-validation runs
    f1_score : float
        F1-score averaged over cross-validation runs
    """
    # Initialize performance measures (will be average over validation sets)
    confmat = np.zeros((2,2))
    f1_score = 0.0
    count = 0

    # Do 5-fold cross-validation
    kf = KFold(n_splits=cv_num_split, shuffle=True)
    clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, metric='minkowski', p=2)
    for train_index, test_index in kf.split(X):
        # Fit and test model
        clf.fit(X[train_index], y[train_index])
        y_hat = clf.predict(X[test_index])
        
        # Compute performance measures
        f1_score += metrics.f1_score(y[test_index], y_hat)
        confmat += metrics.confusion_matrix(y[test_index], y_hat)
        count += 1

    # Average performance measures
    f1_score /= count
    confmat /= count

    return clf, confmat, f1_score